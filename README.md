# Ingest√£o e Busca Sem√¢ntica com LangChain e Postgres (pgVector) ¬∑ PT/EN

> **PT:** Starter minimalista que ingere um PDF em um banco PostgreSQL com pgVector e realiza buscas sem√¢nticas via LangChain, respondendo no terminal com base **apenas** no conte√∫do do PDF.
>
> **EN:** Minimal starter that ingests a PDF into PostgreSQL with pgVector and performs semantic search via LangChain, answering in the terminal using **only** the PDF content.

---

## ‚úÖ Objetivo (PT)
- **Ingest√£o:** Ler um PDF, dividir em chunks e salvar embeddings no Postgres (pgVector).
- **Busca:** Perguntar via CLI e receber respostas **somente** com base no conte√∫do do PDF.

## ‚úÖ Goal (EN)
- **Ingestion:** Read a PDF, split into chunks, and store embeddings in Postgres (pgVector).
- **Search:** Ask via CLI and get answers **only** based on the PDF content.

---

## üß∞ Tecnologias / Stack
- **Python**
- **LangChain**
- **PostgreSQL + pgVector**
- **Docker & Docker Compose** (para subir o banco / to start the DB)

### Pacotes recomendados (PT/EN)
- Split: `from langchain_text_splitters import RecursiveCharacterTextSplitter`
- Embeddings (OpenAI): `from langchain_openai import OpenAIEmbeddings`
- Embeddings (Gemini): `from langchain_google_genai import GoogleGenerativeAIEmbeddings`
- PDF: `from langchain_community.document_loaders import PyPDFLoader`
- Vetor store: `from langchain_postgres import PGVector`

---

## üìÅ Estrutura do projeto / Project structure

```
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ prompts
‚îÇ   ‚îú‚îÄ‚îÄ qa_prompt_en.txt  #pronpt in English
‚îÇ   ‚îî‚îÄ‚îÄ qa_prompt_pt.txt  #prompt in Portuguese
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py      # Ingest√£o do PDF / PDF ingestion
‚îÇ   ‚îú‚îÄ‚îÄ common.py      # Utilidades / Utility functions
‚îÇ   ‚îú‚îÄ‚îÄ search.py      # Busca no banco vetorial / Semantic search
‚îÇ   ‚îú‚îÄ‚îÄ chat.py        # CLI para intera√ß√£o / CLI chat
‚îú‚îÄ‚îÄ document.pdf       # Adicione seu PDF aqui / Place your PDF here
‚îî‚îÄ‚îÄ README.md
```

---

## üå± Setup

### 1) VirtualEnv (Unix-like)
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 1) VirtualEnv (Windows PowerShell)
```powershell
py -m venv venv
venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

### 2) Banco de Dados / Database
Suba o Postgres com pgVector:
```bash
docker compose up -d
```

### 3) Configura√ß√£o / Configuration
Copie `.env.example` para `.env` e preencha as chaves:
```bash
cp .env.example .env  # Windows: copy .env.example .env
```

- Se usar **OpenAI**:
  - `EMBEDDINGS_PROVIDER=openai`
  - `LLM_PROVIDER=openai`
  - `OPENAI_API_KEY=...`
  - (padr√µes: `text-embedding-3-small`, `gpt-5-nano`)

- Se usar **Gemini**:
  - `EMBEDDINGS_PROVIDER=gemini`
  - `LLM_PROVIDER=gemini`
  - `GOOGLE_API_KEY=...`
  - (padr√µes: `models/embedding-001`, `gemini-2.5-flash-lite`)

> Observa√ß√£o: os nomes de modelos acima seguem sua sugest√£o. Se sua conta n√£o suportar tais modelos, ajuste no `.env`.

### 4) Adicione o PDF
Coloque seu arquivo como `./document.pdf`.

---

## ‚ñ∂Ô∏è Execu√ß√£o / Run

### Ingest√£o do PDF
```bash
python src/ingest.py --pdf ./document.pdf
```

### Chat no terminal
```bash
python src/chat.py
```
#### Obs: Caso queira iniciar o chat sem fazer a ingest√£o de dados, o chat ir√° perguntar sem que quer fazer a ingestao de dados.

Exemplo no CLI (PT):
```
PERGUNTA: Qual o faturamento da Empresa SuperTechIABrazil?
RESPOSTA: O faturamento foi de 10 milh√µes de reais.
```

Perguntas fora do contexto (PT):
```
PERGUNTA: Quantos clientes temos em 2024?
RESPOSTA: N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta.
```

---

## üß™ Como funciona (resumo) / How it works (summary)
- O `ingest.py` l√™ o PDF, divide em **chunks de 1000** com **overlap 150**, gera embeddings com o provedor escolhido e persiste em uma cole√ß√£o no Postgres/pgVector.
- O `search.py` executa `similarity_search_with_score(query, k=10)` na cole√ß√£o.
- O `chat.py` monta o prompt com o **CONTEXTO** (top K) e chama a LLM. As regras impedem alucina√ß√£o: se n√£o houver evid√™ncia no contexto, responde a negativa padronizada.

---

## ü™™ Licen√ßa / License
MIT
